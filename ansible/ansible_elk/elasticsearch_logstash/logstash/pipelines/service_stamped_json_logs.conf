# Логи будут прилетать из beats по порту 5044
input {
    beats {
        port => 5044
    }
}

filter {
    # Дропаем лог, если он пришел неизвестного нам сервиса (по желанию)
    # Поле service у нас появится благодаря конфигурированию filebeat

    if [fields][service] not in ["host_metrics_app", "nginx_log"] {
        drop {}
    }
    
    # Оригинальный json-log, который был сгенерирован вашим приложением, будет лежать по ключу message
    # (из filebeat'а логи прилетают не в чистом виде)
    json {
        source => "message"
    }

    # Говорим logstash'у, чтобы в качестве timestamp'а логи он брал именно наш timestamp
    # (в моем случае поле asctime в теле сообщения в формате "yyyy-MM-dd HH:mm:ss.SSS" и часовом поясе UTC)
    # и затем подтирал поле asctime.
    date {
        match => ["asctime", "yyyy-MM-dd HH:mm:ss.SSS"]
        timezone => "UTC"
        target => "@timestamp"
        remove_field => ["asctime"]
    }

    if [fields][service] in ["nginx_log", "nginx_log"] {
        grok {
            match => { "message" => "%{IPORHOST:remote_ip} - - \[%{HTTPDATE:access_time}\] \"%{WORD:http_method} / HTTP/%{NUMBER:http_version}\" %{NUMBER:response_code} %{NUMBER:body_sent_bytes} \"%{DATA:referrer}\" \"%{DATA:agent}\"" }
        }
        
        mutate {
            remove_field => ["host"]
        }
    }
}

output {
    # Отображаем лог в stdout (поиграйтесь и удалите данную строку)
    stdout {}
    # Пушим лог в elasticsearch, индекс будет создан автоматически по названию сервиса и текущей дате
    elasticsearch {
        hosts => "elasticsearch:9200"
        index => "logs_%{[fields][service]}-%{+YYYY.MM.dd}"
        user => "elastic"
        password => "bgkCHk4Y5"
    }
}




